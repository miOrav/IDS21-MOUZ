{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7352078e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-141-25b54ff3c7e2>:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  needed_data['Pawpularity'] = needed_data['Pawpularity'].astype(str)\n",
      "C:\\Users\\uzingel\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n",
      "Epoch 1/200\n",
      "930/930 [==============================] - 44s 47ms/step - loss: 129.2804 - val_loss: 72.5946\n",
      "Epoch 2/200\n",
      "930/930 [==============================] - 47s 51ms/step - loss: 71.8589 - val_loss: 65.4726\n",
      "Epoch 3/200\n",
      "930/930 [==============================] - 45s 49ms/step - loss: 62.4522 - val_loss: 59.2160\n",
      "Epoch 4/200\n",
      "930/930 [==============================] - 44s 48ms/step - loss: 58.9498 - val_loss: 57.4920\n",
      "Epoch 5/200\n",
      "930/930 [==============================] - 45s 49ms/step - loss: 59.0065 - val_loss: 57.5426\n",
      "Epoch 6/200\n",
      "930/930 [==============================] - 44s 48ms/step - loss: 58.6963 - val_loss: 58.2752\n",
      "Epoch 7/200\n",
      "930/930 [==============================] - 46s 50ms/step - loss: 57.8622 - val_loss: 57.9392\n",
      "Epoch 8/200\n",
      "930/930 [==============================] - 48s 51ms/step - loss: 57.6338 - val_loss: 57.7466\n",
      "Epoch 9/200\n",
      "930/930 [==============================] - 44s 47ms/step - loss: 57.5846 - val_loss: 57.5907\n",
      "Epoch 10/200\n",
      "930/930 [==============================] - 44s 47ms/step - loss: 57.5033 - val_loss: 57.5884\n",
      "Epoch 11/200\n",
      "930/930 [==============================] - 44s 48ms/step - loss: 57.3174 - val_loss: 57.5786\n",
      "Epoch 12/200\n",
      "930/930 [==============================] - 45s 49ms/step - loss: 57.3677 - val_loss: 57.2254\n",
      "Epoch 13/200\n",
      "930/930 [==============================] - 44s 47ms/step - loss: 57.3244 - val_loss: 57.3612\n",
      "Epoch 14/200\n",
      "930/930 [==============================] - 45s 48ms/step - loss: 57.2365 - val_loss: 57.2259\n",
      "Epoch 15/200\n",
      "930/930 [==============================] - 42s 45ms/step - loss: 57.0957 - val_loss: 57.5599\n",
      "Epoch 16/200\n",
      "930/930 [==============================] - 44s 47ms/step - loss: 57.1649 - val_loss: 57.0733\n",
      "Epoch 17/200\n",
      "930/930 [==============================] - 46s 50ms/step - loss: 57.2402 - val_loss: 57.1561\n",
      "Epoch 18/200\n",
      "930/930 [==============================] - 46s 50ms/step - loss: 57.1502 - val_loss: 57.2500\n",
      "Epoch 19/200\n",
      "930/930 [==============================] - 45s 48ms/step - loss: 57.1869 - val_loss: 57.1458\n",
      "Epoch 20/200\n",
      "930/930 [==============================] - 45s 48ms/step - loss: 57.1464 - val_loss: 57.2972\n",
      "Epoch 21/200\n",
      "930/930 [==============================] - 44s 47ms/step - loss: 57.1570 - val_loss: 57.1789\n",
      "Epoch 22/200\n",
      "930/930 [==============================] - 45s 48ms/step - loss: 57.1129 - val_loss: 57.1846\n",
      "Epoch 23/200\n",
      "930/930 [==============================] - 44s 47ms/step - loss: 57.1447 - val_loss: 57.2106\n",
      "Epoch 24/200\n",
      "930/930 [==============================] - 42s 45ms/step - loss: 57.1681 - val_loss: 57.1784\n",
      "Epoch 25/200\n",
      "930/930 [==============================] - 43s 46ms/step - loss: 57.1119 - val_loss: 57.1642\n",
      "Epoch 26/200\n",
      "930/930 [==============================] - 42s 45ms/step - loss: 57.1737 - val_loss: 57.1776\n",
      "Epoch 27/200\n",
      "930/930 [==============================] - 42s 45ms/step - loss: 57.1295 - val_loss: 57.3754\n",
      "Epoch 28/200\n",
      "930/930 [==============================] - 41s 44ms/step - loss: 57.1321 - val_loss: 57.1302\n",
      "Epoch 29/200\n",
      "930/930 [==============================] - 41s 44ms/step - loss: 57.1076 - val_loss: 57.1809\n",
      "Epoch 30/200\n",
      "930/930 [==============================] - 41s 44ms/step - loss: 57.1387 - val_loss: 57.1770\n",
      "Epoch 31/200\n",
      "930/930 [==============================] - 42s 45ms/step - loss: 57.1286 - val_loss: 57.1631\n",
      "Epoch 32/200\n",
      "930/930 [==============================] - 42s 45ms/step - loss: 57.1208 - val_loss: 57.2950\n",
      "Epoch 33/200\n",
      "930/930 [==============================] - 41s 44ms/step - loss: 57.1137 - val_loss: 57.4067\n",
      "Epoch 34/200\n",
      "930/930 [==============================] - 44s 47ms/step - loss: 57.1086 - val_loss: 57.4562\n",
      "Epoch 35/200\n",
      "930/930 [==============================] - 43s 46ms/step - loss: 57.1347 - val_loss: 57.1757\n",
      "Epoch 36/200\n",
      "930/930 [==============================] - 46s 49ms/step - loss: 57.0893 - val_loss: 57.1494\n",
      "Epoch 37/200\n",
      "930/930 [==============================] - 40s 43ms/step - loss: 57.1172 - val_loss: 57.1253\n",
      "Epoch 38/200\n",
      "930/930 [==============================] - 43s 46ms/step - loss: 57.1152 - val_loss: 57.5043\n",
      "Epoch 39/200\n",
      "930/930 [==============================] - 40s 43ms/step - loss: 57.1035 - val_loss: 57.4710\n",
      "Epoch 40/200\n",
      "930/930 [==============================] - 41s 44ms/step - loss: 57.1093 - val_loss: 57.1316\n",
      "Epoch 41/200\n",
      "930/930 [==============================] - 42s 46ms/step - loss: 57.0384 - val_loss: 57.7075\n",
      "Epoch 42/200\n",
      "930/930 [==============================] - 45s 48ms/step - loss: 57.1694 - val_loss: 57.2609\n",
      "Epoch 43/200\n",
      "930/930 [==============================] - 44s 47ms/step - loss: 57.1223 - val_loss: 57.1549\n",
      "Epoch 44/200\n",
      "930/930 [==============================] - 43s 46ms/step - loss: 57.1483 - val_loss: 57.3556\n",
      "Epoch 45/200\n",
      "930/930 [==============================] - 43s 47ms/step - loss: 57.1406 - val_loss: 57.1581\n",
      "Epoch 46/200\n",
      "930/930 [==============================] - 43s 46ms/step - loss: 57.1119 - val_loss: 57.3469\n",
      "Epoch 47/200\n",
      "930/930 [==============================] - 43s 47ms/step - loss: 57.1657 - val_loss: 57.2450\n",
      "Epoch 48/200\n",
      "930/930 [==============================] - 42s 45ms/step - loss: 57.1383 - val_loss: 57.1944\n",
      "Epoch 49/200\n",
      "930/930 [==============================] - 44s 48ms/step - loss: 57.0998 - val_loss: 57.2583\n",
      "Epoch 50/200\n",
      "930/930 [==============================] - 44s 47ms/step - loss: 57.1680 - val_loss: 57.1758\n",
      "Epoch 51/200\n",
      "930/930 [==============================] - 44s 48ms/step - loss: 57.1152 - val_loss: 57.1417\n",
      "Epoch 52/200\n",
      "930/930 [==============================] - 44s 47ms/step - loss: 57.1031 - val_loss: 57.1373\n",
      "Epoch 53/200\n",
      "930/930 [==============================] - 44s 47ms/step - loss: 57.0893 - val_loss: 57.7441\n",
      "Epoch 54/200\n",
      "930/930 [==============================] - 42s 45ms/step - loss: 57.1147 - val_loss: 57.2431\n",
      "Epoch 55/200\n",
      "930/930 [==============================] - 46s 49ms/step - loss: 57.1258 - val_loss: 57.2264\n",
      "Epoch 56/200\n",
      "930/930 [==============================] - 46s 49ms/step - loss: 57.2397 - val_loss: 57.2749\n",
      "Epoch 57/200\n",
      "930/930 [==============================] - 45s 48ms/step - loss: 57.1475 - val_loss: 57.2059\n",
      "Epoch 58/200\n",
      "930/930 [==============================] - 44s 48ms/step - loss: 57.1495 - val_loss: 57.1979\n",
      "Epoch 59/200\n",
      "930/930 [==============================] - 46s 49ms/step - loss: 57.1076 - val_loss: 57.1444\n",
      "Epoch 60/200\n",
      "930/930 [==============================] - 45s 49ms/step - loss: 57.1632 - val_loss: 57.1722\n",
      "Epoch 61/200\n",
      "930/930 [==============================] - 44s 48ms/step - loss: 57.1377 - val_loss: 57.1715\n",
      "Epoch 62/200\n",
      "930/930 [==============================] - 45s 48ms/step - loss: 57.1535 - val_loss: 57.1427\n",
      "Epoch 63/200\n",
      "930/930 [==============================] - 43s 46ms/step - loss: 57.0830 - val_loss: 57.2914\n",
      "Epoch 64/200\n",
      "930/930 [==============================] - 44s 47ms/step - loss: 57.1348 - val_loss: 57.1067\n",
      "Epoch 65/200\n",
      "930/930 [==============================] - 45s 48ms/step - loss: 57.1193 - val_loss: 57.1296\n",
      "Epoch 66/200\n",
      "930/930 [==============================] - 43s 46ms/step - loss: 57.1095 - val_loss: 57.5050\n",
      "Epoch 67/200\n",
      "930/930 [==============================] - 41s 44ms/step - loss: 57.1463 - val_loss: 57.1657\n",
      "Epoch 68/200\n",
      "930/930 [==============================] - 43s 46ms/step - loss: 57.1485 - val_loss: 57.1320\n",
      "Epoch 69/200\n",
      "930/930 [==============================] - 45s 48ms/step - loss: 57.1145 - val_loss: 57.1926\n",
      "Epoch 70/200\n",
      "930/930 [==============================] - 44s 47ms/step - loss: 57.1221 - val_loss: 57.1150\n",
      "Epoch 71/200\n",
      "930/930 [==============================] - 44s 47ms/step - loss: 57.1467 - val_loss: 57.3400\n",
      "Epoch 72/200\n",
      "930/930 [==============================] - 43s 47ms/step - loss: 57.1142 - val_loss: 57.1728\n",
      "Epoch 73/200\n",
      "930/930 [==============================] - 43s 47ms/step - loss: 57.1269 - val_loss: 57.2329\n",
      "Epoch 74/200\n",
      "930/930 [==============================] - 45s 49ms/step - loss: 57.0943 - val_loss: 57.1165\n",
      "Epoch 75/200\n",
      "930/930 [==============================] - 49s 53ms/step - loss: 57.1154 - val_loss: 57.1194\n",
      "Epoch 76/200\n",
      "930/930 [==============================] - 41s 44ms/step - loss: 57.1472 - val_loss: 57.1206\n",
      "Epoch 77/200\n",
      "930/930 [==============================] - 49s 53ms/step - loss: 57.1331 - val_loss: 57.1420\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "930/930 [==============================] - 43s 46ms/step - loss: 57.1392 - val_loss: 57.1379\n",
      "Epoch 79/200\n",
      "930/930 [==============================] - 41s 44ms/step - loss: 57.0012 - val_loss: 57.2450\n",
      "Epoch 80/200\n",
      "930/930 [==============================] - 41s 44ms/step - loss: 57.1725 - val_loss: 57.1816\n",
      "Epoch 81/200\n",
      "930/930 [==============================] - 41s 44ms/step - loss: 57.1508 - val_loss: 57.2091\n",
      "Epoch 82/200\n",
      "930/930 [==============================] - 40s 43ms/step - loss: 57.1376 - val_loss: 57.1548\n",
      "Epoch 83/200\n",
      "930/930 [==============================] - 41s 45ms/step - loss: 57.1134 - val_loss: 57.1823\n",
      "Epoch 84/200\n",
      "930/930 [==============================] - 42s 45ms/step - loss: 57.1060 - val_loss: 57.1350\n",
      "Epoch 85/200\n",
      "930/930 [==============================] - 42s 45ms/step - loss: 57.0655 - val_loss: 57.1374\n",
      "Epoch 86/200\n",
      "930/930 [==============================] - 40s 43ms/step - loss: 57.1364 - val_loss: 57.1371\n",
      "Epoch 87/200\n",
      "930/930 [==============================] - 43s 46ms/step - loss: 57.1520 - val_loss: 57.1610\n",
      "Epoch 88/200\n",
      "930/930 [==============================] - 42s 45ms/step - loss: 57.1127 - val_loss: 57.1322\n",
      "Epoch 89/200\n",
      "930/930 [==============================] - 43s 46ms/step - loss: 57.1374 - val_loss: 57.1284\n",
      "Epoch 90/200\n",
      "930/930 [==============================] - 53s 57ms/step - loss: 57.0877 - val_loss: 57.3232\n",
      "Epoch 91/200\n",
      "930/930 [==============================] - 49s 53ms/step - loss: 57.0794 - val_loss: 57.1386\n",
      "Epoch 92/200\n",
      "930/930 [==============================] - 46s 50ms/step - loss: 57.1159 - val_loss: 57.1382\n",
      "Epoch 93/200\n",
      "930/930 [==============================] - 43s 46ms/step - loss: 57.1319 - val_loss: 57.1126\n",
      "Epoch 94/200\n",
      "930/930 [==============================] - 47s 50ms/step - loss: 57.1335 - val_loss: 57.1777\n",
      "Epoch 95/200\n",
      "930/930 [==============================] - 46s 49ms/step - loss: 57.1021 - val_loss: 57.3239\n",
      "Epoch 96/200\n",
      "930/930 [==============================] - 49s 53ms/step - loss: 57.1485 - val_loss: 57.1744\n",
      "Epoch 97/200\n",
      "930/930 [==============================] - 48s 51ms/step - loss: 57.1346 - val_loss: 57.1487\n",
      "Epoch 98/200\n",
      "930/930 [==============================] - 49s 52ms/step - loss: 57.0747 - val_loss: 57.0956\n",
      "Epoch 99/200\n",
      "930/930 [==============================] - 46s 50ms/step - loss: 57.1206 - val_loss: 57.1480\n",
      "Epoch 100/200\n",
      "930/930 [==============================] - 46s 50ms/step - loss: 57.0806 - val_loss: 57.1920\n",
      "Epoch 101/200\n",
      "930/930 [==============================] - 44s 48ms/step - loss: 57.2117 - val_loss: 57.1632\n",
      "Epoch 102/200\n",
      "930/930 [==============================] - 44s 47ms/step - loss: 57.1296 - val_loss: 57.1496\n",
      "Epoch 103/200\n",
      "930/930 [==============================] - 42s 45ms/step - loss: 57.1094 - val_loss: 57.1947\n",
      "Epoch 104/200\n",
      "930/930 [==============================] - 42s 45ms/step - loss: 57.1095 - val_loss: 57.1582\n",
      "Epoch 105/200\n",
      "930/930 [==============================] - 44s 47ms/step - loss: 57.1243 - val_loss: 57.1930\n",
      "Epoch 106/200\n",
      "930/930 [==============================] - 43s 46ms/step - loss: 57.0969 - val_loss: 57.0955\n",
      "Epoch 107/200\n",
      "930/930 [==============================] - 43s 46ms/step - loss: 57.1216 - val_loss: 57.1100\n",
      "Epoch 108/200\n",
      "930/930 [==============================] - 44s 47ms/step - loss: 57.0472 - val_loss: 57.0933\n",
      "Epoch 109/200\n",
      "930/930 [==============================] - 46s 49ms/step - loss: 57.0905 - val_loss: 57.1167\n",
      "Epoch 110/200\n",
      "930/930 [==============================] - 45s 48ms/step - loss: 57.1032 - val_loss: 57.1287\n",
      "Epoch 111/200\n",
      "930/930 [==============================] - 43s 46ms/step - loss: 57.0126 - val_loss: 57.2830\n",
      "Epoch 112/200\n",
      "930/930 [==============================] - 44s 47ms/step - loss: 57.1179 - val_loss: 57.3186\n",
      "Epoch 113/200\n",
      "930/930 [==============================] - 43s 46ms/step - loss: 57.0608 - val_loss: 57.2141\n",
      "Epoch 114/200\n",
      "930/930 [==============================] - 44s 47ms/step - loss: 57.1212 - val_loss: 57.3370\n",
      "Epoch 115/200\n",
      "930/930 [==============================] - 46s 50ms/step - loss: 57.1016 - val_loss: 57.1123\n",
      "Epoch 116/200\n",
      "930/930 [==============================] - 45s 49ms/step - loss: 56.9981 - val_loss: 57.2291\n",
      "Epoch 117/200\n",
      "930/930 [==============================] - 45s 49ms/step - loss: 57.0326 - val_loss: 57.0168\n",
      "Epoch 118/200\n",
      "930/930 [==============================] - 46s 49ms/step - loss: 56.9728 - val_loss: 57.3693\n",
      "Epoch 119/200\n",
      "930/930 [==============================] - 46s 50ms/step - loss: 56.9426 - val_loss: 57.6258\n",
      "Epoch 120/200\n",
      "930/930 [==============================] - 45s 49ms/step - loss: 57.0031 - val_loss: 56.9354\n",
      "Epoch 121/200\n",
      "930/930 [==============================] - 45s 49ms/step - loss: 57.0062 - val_loss: 56.9829\n",
      "Epoch 122/200\n",
      "930/930 [==============================] - 45s 49ms/step - loss: 57.0560 - val_loss: 57.0220\n",
      "Epoch 123/200\n",
      "930/930 [==============================] - 46s 49ms/step - loss: 57.0360 - val_loss: 57.0057\n",
      "Epoch 124/200\n",
      "930/930 [==============================] - 44s 47ms/step - loss: 56.8856 - val_loss: 56.9423\n",
      "Epoch 125/200\n",
      "930/930 [==============================] - 46s 50ms/step - loss: 56.9075 - val_loss: 57.2373\n",
      "Epoch 126/200\n",
      "930/930 [==============================] - 45s 49ms/step - loss: 56.7968 - val_loss: 57.0779\n",
      "Epoch 127/200\n",
      "930/930 [==============================] - 46s 49ms/step - loss: 56.9057 - val_loss: 57.2485\n",
      "Epoch 128/200\n",
      "930/930 [==============================] - 47s 51ms/step - loss: 56.8875 - val_loss: 57.0242\n",
      "Epoch 129/200\n",
      "930/930 [==============================] - 45s 48ms/step - loss: 56.9201 - val_loss: 57.0402\n",
      "Epoch 130/200\n",
      "930/930 [==============================] - 43s 47ms/step - loss: 56.7494 - val_loss: 57.1504\n",
      "Epoch 131/200\n",
      "930/930 [==============================] - 44s 47ms/step - loss: 56.8811 - val_loss: 57.4170\n",
      "Epoch 132/200\n",
      "930/930 [==============================] - 42s 46ms/step - loss: 56.8931 - val_loss: 58.1433\n",
      "Epoch 133/200\n",
      "930/930 [==============================] - 45s 48ms/step - loss: 56.7941 - val_loss: 57.7139\n",
      "Epoch 134/200\n",
      "930/930 [==============================] - 46s 49ms/step - loss: 56.8955 - val_loss: 57.1980\n",
      "Epoch 135/200\n",
      "930/930 [==============================] - 43s 46ms/step - loss: 56.9929 - val_loss: 57.4811\n",
      "Epoch 136/200\n",
      "930/930 [==============================] - 44s 48ms/step - loss: 56.7657 - val_loss: 57.6033\n",
      "Epoch 137/200\n",
      "930/930 [==============================] - 43s 46ms/step - loss: 56.7677 - val_loss: 57.6090\n",
      "Epoch 138/200\n",
      "930/930 [==============================] - 44s 47ms/step - loss: 56.6154 - val_loss: 57.9171\n",
      "Epoch 139/200\n",
      "930/930 [==============================] - 42s 45ms/step - loss: 56.5787 - val_loss: 57.8926\n",
      "Epoch 140/200\n",
      "930/930 [==============================] - 41s 44ms/step - loss: 56.6218 - val_loss: 59.2539\n",
      "Epoch 141/200\n",
      "930/930 [==============================] - 37s 40ms/step - loss: 56.6931 - val_loss: 58.2013\n",
      "Epoch 142/200\n",
      "930/930 [==============================] - 32s 34ms/step - loss: 56.5185 - val_loss: 57.6377\n",
      "Epoch 143/200\n",
      "930/930 [==============================] - 31s 33ms/step - loss: 56.5462 - val_loss: 57.3322\n",
      "Epoch 144/200\n",
      "930/930 [==============================] - 30s 32ms/step - loss: 56.4788 - val_loss: 58.0414\n",
      "Epoch 145/200\n",
      "930/930 [==============================] - 31s 34ms/step - loss: 56.2354 - val_loss: 58.1450\n",
      "Epoch 146/200\n",
      "930/930 [==============================] - 32s 35ms/step - loss: 56.4762 - val_loss: 57.8544\n",
      "Epoch 147/200\n",
      "930/930 [==============================] - 33s 35ms/step - loss: 56.4302 - val_loss: 57.7780\n",
      "Epoch 148/200\n",
      "930/930 [==============================] - 29s 31ms/step - loss: 56.3176 - val_loss: 58.3750\n",
      "Epoch 149/200\n",
      "930/930 [==============================] - 32s 34ms/step - loss: 56.5797 - val_loss: 57.9858\n",
      "Epoch 150/200\n",
      "930/930 [==============================] - 32s 34ms/step - loss: 56.3645 - val_loss: 58.0240\n",
      "Epoch 151/200\n",
      "930/930 [==============================] - 34s 37ms/step - loss: 56.3633 - val_loss: 58.2190\n",
      "Epoch 152/200\n",
      "930/930 [==============================] - 32s 35ms/step - loss: 56.2082 - val_loss: 57.5713\n",
      "Epoch 153/200\n",
      "930/930 [==============================] - 33s 36ms/step - loss: 56.2934 - val_loss: 58.2034\n",
      "Epoch 154/200\n",
      "930/930 [==============================] - 32s 34ms/step - loss: 56.2243 - val_loss: 57.8884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/200\n",
      "930/930 [==============================] - 32s 35ms/step - loss: 56.0122 - val_loss: 58.2337\n",
      "Epoch 156/200\n",
      "930/930 [==============================] - 32s 35ms/step - loss: 55.6765 - val_loss: 58.4318\n",
      "Epoch 157/200\n",
      "930/930 [==============================] - 34s 37ms/step - loss: 55.7834 - val_loss: 58.5795\n",
      "Epoch 158/200\n",
      "930/930 [==============================] - 33s 35ms/step - loss: 55.7229 - val_loss: 58.5344\n",
      "Epoch 159/200\n",
      "930/930 [==============================] - 34s 37ms/step - loss: 55.2508 - val_loss: 58.6045\n",
      "Epoch 160/200\n",
      "930/930 [==============================] - 33s 36ms/step - loss: 55.5752 - val_loss: 58.5787\n",
      "Epoch 161/200\n",
      "930/930 [==============================] - 35s 38ms/step - loss: 55.2328 - val_loss: 58.8571\n",
      "Epoch 162/200\n",
      "930/930 [==============================] - 33s 36ms/step - loss: 55.7288 - val_loss: 58.1013\n",
      "Epoch 163/200\n",
      "930/930 [==============================] - 32s 35ms/step - loss: 55.0180 - val_loss: 59.0210\n",
      "Epoch 164/200\n",
      "930/930 [==============================] - 34s 37ms/step - loss: 55.1221 - val_loss: 58.4596\n",
      "Epoch 165/200\n",
      "930/930 [==============================] - 33s 36ms/step - loss: 54.7229 - val_loss: 58.9724\n",
      "Epoch 166/200\n",
      "930/930 [==============================] - 32s 34ms/step - loss: 54.8817 - val_loss: 59.3291\n",
      "Epoch 167/200\n",
      "930/930 [==============================] - 30s 32ms/step - loss: 54.7767 - val_loss: 58.9930\n",
      "Epoch 168/200\n",
      "930/930 [==============================] - 29s 32ms/step - loss: 54.8182 - val_loss: 58.9435\n",
      "Epoch 169/200\n",
      "930/930 [==============================] - 29s 32ms/step - loss: 54.4387 - val_loss: 59.4893\n",
      "Epoch 170/200\n",
      "930/930 [==============================] - 31s 33ms/step - loss: 54.1790 - val_loss: 59.6675\n",
      "Epoch 171/200\n",
      "930/930 [==============================] - 32s 35ms/step - loss: 53.8923 - val_loss: 59.9282\n",
      "Epoch 172/200\n",
      "930/930 [==============================] - 31s 33ms/step - loss: 53.7216 - val_loss: 59.7060\n",
      "Epoch 173/200\n",
      "930/930 [==============================] - 35s 38ms/step - loss: 53.9545 - val_loss: 59.5271\n",
      "Epoch 174/200\n",
      "930/930 [==============================] - 43s 46ms/step - loss: 53.9810 - val_loss: 59.8802\n",
      "Epoch 175/200\n",
      "930/930 [==============================] - 40s 43ms/step - loss: 53.6993 - val_loss: 60.4111\n",
      "Epoch 176/200\n",
      "930/930 [==============================] - 37s 40ms/step - loss: 53.6055 - val_loss: 60.1970\n",
      "Epoch 177/200\n",
      "930/930 [==============================] - 43s 47ms/step - loss: 52.4821 - val_loss: 60.8468\n",
      "Epoch 178/200\n",
      "930/930 [==============================] - 42s 45ms/step - loss: 52.9197 - val_loss: 60.8959\n",
      "Epoch 179/200\n",
      "930/930 [==============================] - 40s 43ms/step - loss: 51.5590 - val_loss: 61.9277\n",
      "Epoch 180/200\n",
      "930/930 [==============================] - 41s 44ms/step - loss: 50.8299 - val_loss: 63.8494\n",
      "Epoch 181/200\n",
      "930/930 [==============================] - 41s 44ms/step - loss: 50.9432 - val_loss: 61.6465\n",
      "Epoch 182/200\n",
      "930/930 [==============================] - 41s 44ms/step - loss: 50.7454 - val_loss: 60.8460\n",
      "Epoch 183/200\n",
      "930/930 [==============================] - 44s 47ms/step - loss: 50.8049 - val_loss: 61.2713\n",
      "Epoch 184/200\n",
      "930/930 [==============================] - 34s 37ms/step - loss: 50.1102 - val_loss: 62.1141\n",
      "Epoch 185/200\n",
      "930/930 [==============================] - 31s 34ms/step - loss: 49.8520 - val_loss: 62.0808\n",
      "Epoch 186/200\n",
      "930/930 [==============================] - 31s 33ms/step - loss: 49.3465 - val_loss: 62.5794\n",
      "Epoch 187/200\n",
      "930/930 [==============================] - 31s 33ms/step - loss: 48.8717 - val_loss: 63.4892\n",
      "Epoch 188/200\n",
      "930/930 [==============================] - 31s 33ms/step - loss: 48.7101 - val_loss: 61.5525\n",
      "Epoch 189/200\n",
      "930/930 [==============================] - 31s 34ms/step - loss: 48.3100 - val_loss: 62.5188\n",
      "Epoch 190/200\n",
      "930/930 [==============================] - 31s 34ms/step - loss: 48.6849 - val_loss: 62.1400\n",
      "Epoch 191/200\n",
      "930/930 [==============================] - 32s 34ms/step - loss: 48.2642 - val_loss: 62.3268\n",
      "Epoch 192/200\n",
      "930/930 [==============================] - 32s 34ms/step - loss: 47.6967 - val_loss: 60.7187\n",
      "Epoch 193/200\n",
      "930/930 [==============================] - 31s 33ms/step - loss: 47.0742 - val_loss: 62.2229\n",
      "Epoch 194/200\n",
      "930/930 [==============================] - 31s 33ms/step - loss: 47.1569 - val_loss: 61.4255\n",
      "Epoch 195/200\n",
      "930/930 [==============================] - 32s 34ms/step - loss: 46.7381 - val_loss: 62.6951\n",
      "Epoch 196/200\n",
      "930/930 [==============================] - 33s 36ms/step - loss: 46.8440 - val_loss: 61.5819\n",
      "Epoch 197/200\n",
      "930/930 [==============================] - 32s 34ms/step - loss: 47.1281 - val_loss: 62.3736\n",
      "Epoch 198/200\n",
      "930/930 [==============================] - 32s 34ms/step - loss: 46.8603 - val_loss: 62.7501\n",
      "Epoch 199/200\n",
      "930/930 [==============================] - 32s 34ms/step - loss: 46.0896 - val_loss: 62.2933\n",
      "Epoch 200/200\n",
      "930/930 [==============================] - 32s 35ms/step - loss: 45.5087 - val_loss: 62.4401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ffa1bfbb20>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras.layers import Dense, Input, Dropout, GlobalAveragePooling2D, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import argparse\n",
    "import locale\n",
    "\n",
    "\n",
    "\n",
    "test_data = pd.read_csv('test.csv') \n",
    "train_data = pd.read_csv('train.csv') \n",
    "\n",
    "# default image size: 48*48 pixels\n",
    "pic_size = 48\n",
    "\n",
    "# input path for the images\n",
    "base_path = \"../IDS2021_HW09/\"\n",
    "\n",
    "\n",
    "#We select the necessary metadata and convert it into the necessary form\n",
    "needed_data = train_data[['Id', 'Pawpularity']]\n",
    "size = len(needed_data)\n",
    "needed_data['Pawpularity'] = needed_data['Pawpularity'].astype(str)\n",
    "i=0\n",
    "for expression in os.listdir(base_path + \"files/train/\"):\n",
    "    needed_data['Id'].iloc[i] = expression\n",
    "    i+=1\n",
    "\n",
    "\n",
    "\n",
    "#We load all the images into an array\n",
    "images = []\n",
    "for filename in needed_data['Id']:\n",
    "    image = cv2.imread(base_path + \"files/train/\" + filename)\n",
    "    image = cv2.resize(image, (64, 64))\n",
    "    images.append(image)\n",
    "\n",
    "    \n",
    "image_array = np.array(images)\n",
    "\n",
    "    \n",
    "#funtion to make the neural network\n",
    "def make_cnn_model(image_width, image_height, image_depth, filters=(16, 32, 64), regress=False):\n",
    "    Shape = (image_height, image_width, image_depth)\n",
    "    inputs = Input(shape=Shape)\n",
    "    for (i, f) in enumerate(filters):\n",
    "        if i == 0:\n",
    "            x = inputs\n",
    "        x = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization(axis=-1)(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(20)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(8)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    if regress:\n",
    "        x = Dense(1, activation=\"linear\")(x)\n",
    "    model = Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "image_array = image_array /255.0\n",
    "split = train_test_split(train_data['Pawpularity'], image_array, test_size=0.25, random_state=40)\n",
    "(train, test, trainImages, testImages) = split\n",
    "\n",
    "\n",
    "#We convert the pawpularity to be between [0,1]\n",
    "maxPrice = train.max()\n",
    "trainY = train / maxPrice\n",
    "testY = test / maxPrice\n",
    "#We make and train the model\n",
    "model2 = make_cnn_model(64, 64, 3, regress=True)\n",
    "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "model2.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n",
    "\n",
    "print(\"[INFO] training model...\")\n",
    "model2.fit(x=trainImages, y=trainY, \n",
    "    validation_data=(testImages, testY),\n",
    "    epochs=200, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aed5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the values of images\n",
    "preds = model2.predict(testImages)\n",
    "\n",
    "#Calculate the differences between results and the real values\n",
    "diff = preds.flatten() - testY\n",
    "percentDiff = (diff / testY) * 100\n",
    "absPercentDiff = np.abs(percentDiff)\n",
    "\n",
    "#Calculate the mean of our prediction accuracy\n",
    "mean = np.mean(absPercentDiff)\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "57444ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the values for the test data\n",
    "\n",
    "needed_data_test = test_data\n",
    "j = 0\n",
    "for expression in os.listdir(base_path + \"test/\"):\n",
    "    needed_data_test['Id'].iloc[j] = expression\n",
    "    j+=1\n",
    "\n",
    "\n",
    "images2 = []\n",
    "for filename in needed_data_test['Id']:\n",
    "    image2 = cv2.imread(base_path + \"test/\" + filename)\n",
    "    image2 = cv2.resize(image2, (64, 64))\n",
    "    images2.append(image2)\n",
    "\n",
    "    \n",
    "image_array2 = np.array(images2)\n",
    "image_array2 = image_array2 /255.0\n",
    "\n",
    "#Results\n",
    "predict2 = predict2 * 100\n",
    "predict2 = model2.predict(image_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2386cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Here we work with the metadata\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#We split the data into validation and training\n",
    "validation = train_data.iloc[0:40]\n",
    "training = train_data.iloc[40:]\n",
    "\n",
    "#We assign different types of data to variables\n",
    "y_train = training['Pawpularity']\n",
    "X_train = training.drop(columns=['Pawpularity', 'Id'])\n",
    "X_test = validation.drop(columns=['Id', 'Pawpularity'])\n",
    "y_test = validation['Pawpularity']\n",
    "\n",
    "#Treeclassifier\n",
    "model3 = tree.DecisionTreeClassifier(random_state=0, criterion='gini')\n",
    "model3.fit(X_train, y_train)\n",
    "results = model3.predict(X_test)\n",
    "accuracy1 = accuracy_score(y_test, model3.predict(X_test))\n",
    "#print(results)\n",
    "\n",
    "#KN\n",
    "model4 = KNeighborsClassifier(n_neighbors = 11)\n",
    "model4.fit(X_train, y_train)\n",
    "results2 = model4.predict(X_test)\n",
    "accuracy2 = accuracy_score(y_test, model4.predict(X_test))\n",
    "#print(results2)\n",
    "\n",
    "#KN with manhattan metric\n",
    "model5 = KNeighborsClassifier(n_neighbors = 11, metric=\"manhattan\")\n",
    "model5.fit(X_train, y_train)\n",
    "results3 = model5.predict(X_test)\n",
    "accuracy3 = accuracy_score(y_test, model5.predict(X_test))\n",
    "#print(results3)\n",
    "\n",
    "#print(y_test)\n",
    "\n",
    "\n",
    "#As we can see, the accuracy is very low with just the metadata\n",
    "print(accuracy1)\n",
    "print(accuracy2)\n",
    "print(accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95fb4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns=['model', 'accuracy'])\n",
    "results_df = results_df.append({'model': 'DecisionTreeClassifier', 'accuracy': accuracy1 }, ignore_index=True)\n",
    "results_df = results_df.append({'model': 'KNeighborsClassifier', 'accuracy': accuracy2 }, ignore_index=True)\n",
    "results_df = results_df.append({'model': 'KNeighborsClassifier(manhattan)', 'accuracy': accuracy3 }, ignore_index=True)\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
