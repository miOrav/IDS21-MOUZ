{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5e551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras.layers import Dense, Input, Dropout, GlobalAveragePooling2D, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D, AveragePooling2D, LayerNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import argparse\n",
    "import locale\n",
    "import math\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "needed_data = train_data[['Id', 'Pawpularity']]\n",
    "\n",
    "# Uncomment to get an overview of min, max, mean and main percentile values\n",
    "# needed_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a29dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input path for the images\n",
    "base_path = \"../IDS2021_HW09/\"\n",
    "image_size = (64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0a8850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We load all the images into an array\n",
    "images = []\n",
    "for i, filename in enumerate(os.listdir(base_path + \"files/train/\")):\n",
    "    needed_data['Id'].iloc[i] = filename\n",
    "    image = cv2.imread(base_path + \"files/train/\" + filename)\n",
    "    image = cv2.resize(image, image_size)\n",
    "    images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171888a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to see smallest and largest pictures dimensions\n",
    "# NOTE: previous cell has to be run without the resize part\n",
    "\n",
    "# is1, s1 = 0, 1500\n",
    "# is2, s2 = 0, 1500\n",
    "# il1, l1 = 0, 0\n",
    "# il2, l2 = 0, 0\n",
    "# for i, image in enumerate(images):\n",
    "#     x1, x2, _ = image.shape\n",
    "#     if x1 < s1:\n",
    "#         s1 = x1\n",
    "#         is1 = i\n",
    "#     if x1 > l1:\n",
    "#         l1 = x1\n",
    "#         il1 = i\n",
    "#     if x2 < s2:\n",
    "#         s2 = x2\n",
    "#         is2 = i\n",
    "#     if x2 > l2:\n",
    "#         l2 = x2\n",
    "#         il2 = i\n",
    "# for i in [is1, is2, il1, il2]:\n",
    "#     print(images[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beb78b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_array = np.array(images)\n",
    "image_array = image_array / 255.0 # normalizing values \n",
    "trainX, testX, train, test = train_test_split(image_array, train_data['Pawpularity'], test_size=0.2, random_state=40)\n",
    "valsplit = round(len(train)*0.125)\n",
    "val = train[0:valsplit]\n",
    "train = train[valsplit:]\n",
    "valX = trainX[0:valsplit]\n",
    "trainX = trainX[valsplit:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a5711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks to see if the sets are similar enough to not cause problems due to unbalanced partitioning\n",
    "\n",
    "print(train.mean())\n",
    "print(test.mean())\n",
    "print(val.mean())\n",
    "\n",
    "print(train.max())\n",
    "print(test.max())\n",
    "print(val.max())\n",
    "\n",
    "print(train.min())\n",
    "print(test.min())\n",
    "print(val.min())\n",
    "\n",
    "print(train.std())\n",
    "print(test.std())\n",
    "print(val.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95067dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion to make the neural network\n",
    "def make_cnn_model(image_width, image_height, image_depth, filters=(16, 32, 64), regress=False):\n",
    "        Shape = (image_height, image_width, image_depth)\n",
    "        inputs = Input(shape=Shape)\n",
    "        for (i, f) in enumerate(filters):\n",
    "            if i == 0:\n",
    "                x = inputs\n",
    "            x = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
    "            x = Activation(\"relu\")(x)\n",
    "            x = BatchNormalization(axis=-1)(x)\n",
    "            x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(20)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization(axis=-1)(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(8)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        if regress:\n",
    "            x = Dense(1, activation=\"linear\")(x)\n",
    "        model = Model(inputs, x)\n",
    "        return model\n",
    "\n",
    "def model(shape, loss, opt):\n",
    "    m = Sequential()\n",
    "    m.add(Input(shape))\n",
    "    for i in range(2,5):\n",
    "        m.add(Conv2D((math.pow(2,i))*16, kernel_size=(3, 3), padding=\"valid\", activation=\"relu\"))\n",
    "        m.add(LayerNormalization(axis=-1))\n",
    "        m.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    m.add(Flatten())\n",
    "    m.add(Dense(20, activation=\"relu\"))\n",
    "    m.add(Dense(10, activation=\"elu\"))\n",
    "    m.add(BatchNormalization(axis=-1))\n",
    "    m.add(Dense(1000, activation=\"relu\"))\n",
    "    m.add(LayerNormalization(axis=-1))\n",
    "    m.add(Dense(10, activation=\"elu\"))\n",
    "#     m.add(Dropout(0.1))\n",
    "    m.add(Dense(5))\n",
    "    m.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    m.compile(loss=loss, optimizer=opt)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bebba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We convert the pawpularity to be between [0,1]\n",
    "# maxVal = train_data[\"Pawpularity\"].max()\n",
    "maxVal = 100.0\n",
    "# print(maxVal)\n",
    "trainY = train / maxVal\n",
    "testY = test / maxVal\n",
    "valY = val / maxVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa57f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "valY[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c09523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callbacks and model saving\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=3, restore_best_weights=True),\n",
    "    ModelCheckpoint(base_path + \"saved_models/model.{epoch:02d}-{val_loss:.4f}.hdf5\", save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac38fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5939de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We make and train a model\n",
    "a = trainX[0].shape\n",
    "model2 = make_cnn_model(a[0], a[1], a[2], regress=True)\n",
    "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "\n",
    "model2.compile(loss=\"mean_absolute_error\", optimizer=opt)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b33162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] training model...\")\n",
    "model2.fit(x=trainX, y=trainY, \n",
    "    validation_data=(testX, testY),\n",
    "    epochs=3, batch_size=1024, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4768915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#And another model\n",
    "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "model3 = model(trainX[0].shape, \"mean_absolute_error\", opt=opt)\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd967560",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model3.fit(trainX, \n",
    "                  trainY, \n",
    "                  validation_data=(testX, testY), \n",
    "                  epochs=1, \n",
    "                  batch_size=1024,\n",
    "                  callbacks=callbacks,\n",
    "                 verbose=True)\n",
    "\n",
    "res = pd.DataFrame()\n",
    "preds2 = model3.predict(testX)\n",
    "res[\"model2\"] = pd.Series(list(preds2[0:10]))\n",
    "res[\"real\"] = pd.Series(list(valY[0:10]))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7352078e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aed5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Predict the values of images\n",
    "preds1 = model2.predict(valX)\n",
    "\n",
    "# #Calculate the differences between results and the real values\n",
    "diff = preds1.flatten() - valY\n",
    "percentDiff = (diff / valY) * 100\n",
    "absPercentDiff = np.abs(percentDiff)\n",
    "\n",
    "#Calculate the mean of our prediction accuracy\n",
    "mean = np.mean(absPercentDiff)\n",
    "print(\"Model1 \", mean)\n",
    "\n",
    "# Predict the values of images\n",
    "preds2 = model3.predict(testX)\n",
    "\n",
    "#Calculate the differences between results and the real values\n",
    "diff = preds2.flatten() - testY\n",
    "percentDiff = (diff / testY) * 100\n",
    "absPercentDiff = np.abs(percentDiff)\n",
    "\n",
    "#Calculate the mean of our prediction accuracy\n",
    "mean = np.mean(absPercentDiff)\n",
    "print(\"Model2 \", mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9380e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()\n",
    "res[\"model1\"] = pd.Series(list(preds1[0:10]))\n",
    "res[\"model2\"] = pd.Series(list(preds2[0:10]))\n",
    "res[\"real\"] = pd.Series(list(valY[0:10]))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57444ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the values for the test data\n",
    "\n",
    "needed_data_test = test_data\n",
    "for i, expression in enumerate(os.listdir(base_path + \"test/\")):\n",
    "    needed_data_test['Id'].iloc[i] = expression\n",
    "\n",
    "images2 = []\n",
    "for filename in needed_data_test['Id']:\n",
    "    image2 = cv2.imread(base_path + \"test/\" + filename)\n",
    "    image2 = cv2.resize(image2, image_size)\n",
    "    images2.append(image2)\n",
    "\n",
    "    \n",
    "image_array2 = np.array(images2)\n",
    "image_array2 = image_array2 /255.0\n",
    "\n",
    "#Results\n",
    "predict2 = model2.predict(image_array2)\n",
    "predict2 = predict2 * 100\n",
    "print(predict2)\n",
    "\n",
    "predict3 = model3.predict(image_array2)\n",
    "predict3 = predict3 * 100\n",
    "print(predict3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2386cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we work with the metadata\n",
    "\n",
    "#Split the data into correct segments\n",
    "validation = train_data.iloc[0:1000]\n",
    "training = train_data.iloc[1000:]\n",
    "y_train = training['Pawpularity']\n",
    "X_train = training.drop(columns=['Pawpularity', 'Id'])\n",
    "X_test = validation.drop(columns=['Id', 'Pawpularity'])\n",
    "y_test = validation['Pawpularity']\n",
    "\n",
    "\n",
    "#DecisionTreeRegression model\n",
    "model_reg1 = tree.DecisionTreeRegressor(random_state=1)\n",
    "model_reg1.fit(X_train, y_train)\n",
    "results1 = model_reg1.predict(X_test)\n",
    "diff = results1 - y_test\n",
    "percentDiff = (diff / y_test) * 100\n",
    "absPercentDiff = np.abs(percentDiff)\n",
    "\n",
    "#Calculate the mean of our prediction accuracy\n",
    "mean_tree = np.mean(absPercentDiff)\n",
    "print(mean_tree)\n",
    "\n",
    "#First KNeighborsRegression model\n",
    "model_reg2 = KNeighborsRegressor(n_neighbors = 1)\n",
    "model_reg2.fit(X_train, y_train)\n",
    "results2 = model_reg2.predict(X_test)\n",
    "diff2 = results2 - y_test\n",
    "percentDiff2 = (diff2 / y_test) * 100\n",
    "absPercentDiff2 = np.abs(percentDiff2)\n",
    "\n",
    "#Calculate the mean of our prediction accuracy\n",
    "meanKN_1 = np.mean(absPercentDiff2)\n",
    "print(meanKN_1)\n",
    "\n",
    "#Second KNeighborsRegression model\n",
    "model_reg3 = KNeighborsRegressor(n_neighbors = 15)\n",
    "model_reg3.fit(X_train, y_train)\n",
    "results3 = model_reg3.predict(X_test)\n",
    "diff3 = results3 - y_test\n",
    "percentDiff3 = (diff3 / y_test) * 100\n",
    "absPercentDiff3 = np.abs(percentDiff3)\n",
    "\n",
    "#Calculate the mean of our prediction accuracy\n",
    "meanKN_2 = np.mean(absPercentDiff3)\n",
    "print(meanKN_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da92a0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a neural network for metadata analysis\n",
    "\n",
    "def model_for_metadata(dim, regress=False):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "    if regress:\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95fb4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the training data\n",
    "(train1, test1) = train_test_split(train_data, test_size=0.25, random_state=42)\n",
    "\n",
    "#Normalize the pawpularity\n",
    "maxPrice_metadata = train1[\"Pawpularity\"].max()\n",
    "trainY_metadata = train1[\"Pawpularity\"] / maxPrice_metadata\n",
    "testY_metadata = test1[\"Pawpularity\"] / maxPrice_metadata\n",
    "\n",
    "#Drop unnecessary colmuns\n",
    "trainX_metadata = train1.drop(columns=['Id', 'Pawpularity'])\n",
    "testX_metadata = test1.drop(columns=['Id', 'Pawpularity'])\n",
    "\n",
    "#Create and compile model\n",
    "model_for_metadata = model_for_metadata(trainX_metadata.shape[1], regress=True)\n",
    "model_for_metadata.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952c1d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "model_for_metadata.fit(x=trainX_metadata, y=trainY_metadata, \n",
    "    validation_data=(testX_metadata, testY_metadata),\n",
    "    epochs=100, batch_size=1024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
